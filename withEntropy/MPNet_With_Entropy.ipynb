{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install --upgrade pip\n",
    "# ! pip install --upgrade jupyter ipywidgets\n",
    "# ! pip install mysql-connector-python ollama scikit-learn transformers torch torchvision torchaudio requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "import ollama\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MPNet model for sentence embeddings\n",
    "model = SentenceTransformer('all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Connect to MySQL database and extract the schema\n",
    "database = \"VenueScope\"\n",
    "\n",
    "db = mysql.connector.connect(\n",
    "    host=\"localhost\",\n",
    "    user=\"root\",\n",
    "    password=\"Karaikudi-630002\",\n",
    "    database=database\n",
    ")\n",
    "cursor = db.cursor()\n",
    "\n",
    "query = f\"\"\"\n",
    "SELECT \n",
    "    TABLE_NAME, \n",
    "    COLUMN_NAME \n",
    "FROM \n",
    "    INFORMATION_SCHEMA.COLUMNS \n",
    "WHERE \n",
    "    TABLE_SCHEMA = '{database}'\n",
    "ORDER BY \n",
    "    TABLE_NAME, ORDINAL_POSITION;\n",
    "\"\"\"\n",
    "cursor.execute(query)\n",
    "schemaColumns = cursor.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Process schema to extract table and column names\n",
    "schemaInfo = {}\n",
    "for tableName, columnName in schemaColumns:\n",
    "    if tableName not in schemaInfo:\n",
    "        schemaInfo[tableName] = []\n",
    "    schemaInfo[tableName].append(columnName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizeQuery(query):\n",
    "    \"\"\"\n",
    "    Tokenize the query\n",
    "    \"\"\"\n",
    "    words = query.lower().split()  # Simple tokenization\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to get MPNet embeddings\n",
    "def get_MPNet_embeddings(text):\n",
    "    embedding = model.encode(text, convert_to_numpy=True)\n",
    "    return embedding.reshape(1, -1)  # Ensure it's 2D for cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Convert table and column names to BERT embeddings\n",
    "def getSchemaEmbeddings(schemaInfo):\n",
    "    \"\"\"\n",
    "    Convert schema information (table and column names) into BERT embeddings.\n",
    "    \"\"\"\n",
    "    schemaEmbeddings = []\n",
    "    tableKeys = []\n",
    "    \n",
    "    for table, columns in schemaInfo.items():\n",
    "        for column in columns:\n",
    "            text = f\"{table} {column}\"\n",
    "            embedding = get_MPNet_embeddings(text)\n",
    "            schemaEmbeddings.append(embedding)\n",
    "            tableKeys.append(table)\n",
    "    \n",
    "    return schemaEmbeddings, tableKeys\n",
    "\n",
    "schemaEmbeddings, tableKeys = getSchemaEmbeddings(schemaInfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rankSchemas_V2(userKeywords, schemaInfo, userQueryEmbedding, schemaEmbeddings, tableKeys):\n",
    "    \"\"\"\n",
    "    Rank schema tables based on substring matching and BERT embeddings.\n",
    "    Priority is given to matches of user keywords, with embeddings as a secondary score.\n",
    "    \"\"\"\n",
    "    # Initialize scores\n",
    "    tableScores = {}\n",
    "\n",
    "    # Extract relevant keywords from the user query\n",
    "    relevantKeywords = set(keyword.lower() for keyword in userKeywords)\n",
    "\n",
    "    # Step 1: Apply string matching to prioritize relevant tables\n",
    "    for table in schemaInfo:\n",
    "        columns = schemaInfo[table]\n",
    "        tableLower = table.lower()\n",
    "\n",
    "        # Boost for matches of relevant keywords in table name\n",
    "        for keyword in relevantKeywords:\n",
    "            if keyword in tableLower:\n",
    "                tableScores[table] = tableScores.get(table, 0) + 2\n",
    "        \n",
    "        # Boost for matches of relevant keywords in column names\n",
    "        for column in columns:\n",
    "            for keyword in relevantKeywords:\n",
    "                if keyword in column.lower():\n",
    "                    tableScores[table] = tableScores.get(table, 0) + 2  # Adjust boost as needed\n",
    "\n",
    "    # Step 2: Apply embedding similarity as secondary ranking factor\n",
    "    for i, table in enumerate(tableKeys):\n",
    "        similarityScore = cosine_similarity(userQueryEmbedding, schemaEmbeddings[i]).flatten()[0]\n",
    "        tableScores[table] = tableScores.get(table, 0) + similarityScore\n",
    "\n",
    "    # Step 3: Sort the tables based on the combined score (higher is better)\n",
    "    rankedTables = sorted(tableScores.keys(), key=lambda x: tableScores[x], reverse=True)\n",
    "\n",
    "    # Step 4: Remove duplicates, maintaining order\n",
    "    uniqueRankedTables = []\n",
    "    seenTables = set()\n",
    "    for table in rankedTables:\n",
    "        if table not in seenTables:\n",
    "            uniqueRankedTables.append(table)\n",
    "            seenTables.add(table)\n",
    "\n",
    "    return uniqueRankedTables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Construct SQL query dynamically based on top-ranked table and columns\n",
    "def construct_SQL_Query(rankedTableNames, schemaInfo, userQuery, top_N):\n",
    "    \"\"\"\n",
    "    Construct SQL query dynamically using Ollama based on top n-ranked tables and the user's query.\n",
    "    \"\"\"\n",
    "    # Get the top n-ranked tables\n",
    "    topRankedTables = rankedTableNames[:top_N]\n",
    "    \n",
    "    # Collect schema information for the top-ranked tables\n",
    "    schemaInfoAsString = \"\"\n",
    "    for table in topRankedTables:\n",
    "        columns = schemaInfo[table]\n",
    "        schemaInfoAsString += f\"Table {table}: Columns ({', '.join(columns)})\\n\"\n",
    "\n",
    "    # Pass the schema info and user query to Ollama\n",
    "    stream = ollama.chat(\n",
    "        model='duckdb-nsql',\n",
    "        messages=[{'role': 'user', 'content': f\"This is the schema: \\n{schemaInfoAsString}\\n{userQuery}\"}],\n",
    "        stream=True,\n",
    "    )\n",
    "\n",
    "    response = \"\"\n",
    "    for chunk in stream:\n",
    "        response += chunk['message']['content']\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getForeignKeyRelations(cursor, schemaInfo, database):\n",
    "    \"\"\"\n",
    "    Extract foreign key relationships from the INFORMATION_SCHEMA for the given database.\n",
    "    Returns a dictionary mapping tables to their related tables via foreign keys.\n",
    "    \"\"\"\n",
    "    foreignKeyQuery = f\"\"\"\n",
    "    SELECT \n",
    "        TABLE_NAME, \n",
    "        COLUMN_NAME, \n",
    "        REFERENCED_TABLE_NAME, \n",
    "        REFERENCED_COLUMN_NAME\n",
    "    FROM \n",
    "        INFORMATION_SCHEMA.KEY_COLUMN_USAGE \n",
    "    WHERE \n",
    "        TABLE_SCHEMA = '{database}' \n",
    "        AND REFERENCED_TABLE_NAME IS NOT NULL;\n",
    "    \"\"\"\n",
    "    \n",
    "    cursor.execute(foreignKeyQuery)\n",
    "    foreignKeys = cursor.fetchall()\n",
    "    \n",
    "    foreignKeyRelations = {}\n",
    "    for table, column, refTable, refColumn in foreignKeys:\n",
    "        if table not in foreignKeyRelations:\n",
    "            foreignKeyRelations[table] = []\n",
    "        foreignKeyRelations[table].append((column, refTable, refColumn))\n",
    "    \n",
    "    return foreignKeyRelations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rankColumnsByRelevance(userQueryEmbedding, columnNames, columnEmbeddings):\n",
    "    \"\"\"\n",
    "    Compare user query embedding with column embeddings and rank columns based on relevance.\n",
    "    \"\"\"\n",
    "    columnScores = []\n",
    "    userQueryEmbedding = userQueryEmbedding.reshape(1, -1)  # Reshape user query embedding to 2D\n",
    "\n",
    "    for column, embedding in zip(columnNames, columnEmbeddings):\n",
    "        embedding = embedding.reshape(1, -1)  # Reshape column embedding to 2D\n",
    "        # Compute similarity between the user query and each column embedding (cosine similarity)\n",
    "        similarityScore = cosine_similarity(userQueryEmbedding, embedding)[0][0]  # Extract scalar\n",
    "        columnScores.append((column, similarityScore))\n",
    "\n",
    "    # Sort columns by relevance (higher similarity score first)\n",
    "    columnScores.sort(key=lambda x: x[1], reverse=True)\n",
    "    return columnScores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def constructAndExecuteQuery(cursor, rankedTableNames, schemaInfo, userQuery, top_N, maxAttempts=5):\n",
    "    \"\"\"\n",
    "    retry_construct_and_execute_query_with_column_reranking\n",
    "    For each top_N ranked table, rank its columns by relevance to the user query,\n",
    "    re-rank tables based on the relevance of columns, and generate SQL query if relevant.\n",
    "    \"\"\"\n",
    "    attempt = 0\n",
    "    success = False\n",
    "    ollamaQuery = \"\"\n",
    "    userQueryEmbedding = get_MPNet_embeddings(userQuery)  # Embed the user's query\n",
    "\n",
    "    while not success and attempt < maxAttempts:\n",
    "        try:\n",
    "            # Increment attempt count\n",
    "            attempt += 1\n",
    "            print(f\"Attempt {attempt} to generate and execute the query...\")\n",
    "\n",
    "            # Iterate over top-ranked tables to find the most relevant column match\n",
    "            for tableName in rankedTableNames[:top_N]:\n",
    "                columnNames = schemaInfo[tableName]  # Get columns for the table\n",
    "                columnEmbeddings = get_MPNet_embeddings(columnNames)  # Embed the column names\n",
    "\n",
    "                # Rank columns based on their relevance to the user's query\n",
    "                rankedColumns = rankColumnsByRelevance(userQueryEmbedding, columnNames, columnEmbeddings)\n",
    "                print(f\"Ranked columns for table {tableName}: {rankedColumns}\")\n",
    "\n",
    "                # Check if the top-ranked column has sufficient relevance\n",
    "                topColumn, relevanceScore = rankedColumns[0]\n",
    "                print(f\"Top column: {topColumn}, Relevance score: {relevanceScore}\")\n",
    "\n",
    "                if relevanceScore > 0.5:  # Threshold for relevance (can be adjusted)\n",
    "                    print(f\"Proceeding with table {tableName} and top column {topColumn}\")\n",
    "\n",
    "                    # Generate SQL query using Ollama with the relevant table and columns\n",
    "                    ollamaQuery = construct_SQL_Query([tableName], schemaInfo, userQuery, top_N=1)\n",
    "                    print(\"Generated Query from Ollama:\", ollamaQuery)\n",
    "\n",
    "                    # Try executing the query\n",
    "                    cursor.execute(ollamaQuery)\n",
    "                    results = cursor.fetchall()\n",
    "                    success = True  # Mark success if query executes successfully\n",
    "                    break\n",
    "                else:\n",
    "                    print(f\"Relevance score too low for table {tableName}. Trying the next table...\")\n",
    "\n",
    "        except mysql.connector.Error as err:\n",
    "            print(f\"Query execution failed with error: {err}\")\n",
    "            print(\"Re-ranking columns and trying the next table...\")\n",
    "\n",
    "    # If successful, return the results\n",
    "    if success:\n",
    "        print(\"Query executed successfully!\")\n",
    "        return results\n",
    "    else:\n",
    "        print(f\"Failed after {maxAttempts} attempts.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getForeignKeys(cursor, schemaInfo):\n",
    "    \"\"\"\n",
    "    Extract foreign key relationships for the tables in schema_info.\n",
    "    \"\"\"\n",
    "    foreignKeys = {}\n",
    "    for table in schemaInfo.keys():\n",
    "        cursor.execute(f\"\"\"\n",
    "            SELECT \n",
    "                COLUMN_NAME, \n",
    "                REFERENCED_TABLE_NAME, \n",
    "                REFERENCED_COLUMN_NAME\n",
    "            FROM \n",
    "                INFORMATION_SCHEMA.KEY_COLUMN_USAGE\n",
    "            WHERE \n",
    "                TABLE_NAME = '{table}' AND \n",
    "                TABLE_SCHEMA = 'your_database_name' AND \n",
    "                REFERENCED_TABLE_NAME IS NOT NULL;\n",
    "        \"\"\")\n",
    "        foreignKeys[table] = cursor.fetchall()\n",
    "    return foreignKeys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_SQL_QueryForOllama(topTables, schemaInfo, userQuery):\n",
    "    \"\"\"\n",
    "    Construct SQL query using schema information for the top tables.\n",
    "    \"\"\"\n",
    "    schemaInfoAsString = \"\"\n",
    "    for table in topTables:\n",
    "        columns = schemaInfo[table]\n",
    "        schemaInfoAsString += f\"Table {table}: Columns ({', '.join(columns)})\\n\"\n",
    "\n",
    "    # Prepare the final query for Ollama\n",
    "    queryForOllama = f\"{schemaInfoAsString}\\n{userQuery}\"\n",
    "    return queryForOllama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processUserQuery(query):\n",
    "    # Set maximum attempts and number of top tables to return\n",
    "    maxAttempts = 10\n",
    "    top_N = 2\n",
    "\n",
    "    # Extract keywords from the user query\n",
    "    userKeywords = tokenizeQuery(query)\n",
    "    userQueryEmbedding = get_MPNet_embeddings(query)\n",
    "\n",
    "    # Rank the table names based on user query and embeddings\n",
    "    rankedTableNames = rankSchemas_V2(\n",
    "        userKeywords, schemaInfo, userQueryEmbedding, schemaEmbeddings, tableKeys\n",
    "    )\n",
    "\n",
    "    # Select top N tables after re-ranking\n",
    "    topTablesAfterReranking = rankedTableNames[:top_N]\n",
    "\n",
    "    attempt = 0\n",
    "    success = False\n",
    "    responseText = \"\"\n",
    "\n",
    "    # Try executing the query for a maximum of 'max_attempts' times\n",
    "    while not success and attempt < maxAttempts:\n",
    "        attempt += 1\n",
    "        try:\n",
    "            # Increase the number of tables considered by 1 for each attempt\n",
    "            print(f\"Attempt {attempt}: Considering top {top_N} tables\")\n",
    "\n",
    "            # Re-rank tables after the increment\n",
    "            topTablesAfterReranking = rankedTableNames[:top_N]\n",
    "\n",
    "            print(f\"Top tables after re-ranking: {topTablesAfterReranking}\")\n",
    "\n",
    "            # Construct SQL query for Ollama or similar model\n",
    "            ollamaQuery = construct_SQL_QueryForOllama(topTablesAfterReranking, schemaInfo, query)\n",
    "\n",
    "            # Replace with Ollama chat or appropriate model call\n",
    "            stream = ollama.chat(\n",
    "                model='duckdb-nsql',\n",
    "                messages=[{'role': 'user', 'content': ollamaQuery}],\n",
    "                stream=True\n",
    "            )\n",
    "\n",
    "            # Collect the response from the stream\n",
    "            response = \"\"\n",
    "            for chunk in stream:\n",
    "                response += chunk['message']['content']\n",
    "\n",
    "            # Execute the query on the database\n",
    "            cursor.execute(response)\n",
    "            results = cursor.fetchall()\n",
    "\n",
    "            # If successful, set response text and reset top_N\n",
    "            success = True\n",
    "            if success:\n",
    "                responseText = f\"Query Generated: {response}\\nOutput: {str(results)}\\n\"\n",
    "                top_N = 2  # Reset top_N to its initial value\n",
    "\n",
    "        except mysql.connector.Error as err:\n",
    "            top_N += 1\n",
    "            print(f\"Query execution failed with error: {err}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred: {e}\")\n",
    "            if top_N % 2 != 0:\n",
    "                try:\n",
    "                    temp = random.choice([0.05, 0.1, 0.15, 0.2])\n",
    "                    url = \"http://127.0.0.1:11434/api/generate\"\n",
    "                    headers = {\"Content-Type\": \"application/json\"}\n",
    "                    data = {\n",
    "                        \"model\": \"duckdb-nsql\",\n",
    "                        \"prompt\": ollamaQuery,\n",
    "                        \"max_tokens\": 1024,\n",
    "                        \"temperature\": temp,\n",
    "                        \"stream\": True\n",
    "                    }\n",
    "                    response = requests.post(url, headers=headers, data=json.dumps(data), stream=True)\n",
    "                    buffer = \"\"\n",
    "                    for line in response.iter_lines():\n",
    "                        if line:\n",
    "                            try:\n",
    "                                buffer += line.decode('utf-8')\n",
    "                                result_chunk = json.loads(buffer)\n",
    "                                buffer = \"\"\n",
    "                                if \"response\" in result_chunk:\n",
    "                                    response_text += result_chunk[\"response\"]\n",
    "                            except json.JSONDecodeError:\n",
    "                                continue\n",
    "                    cursor.execute(response_text)\n",
    "                    results = cursor.fetchall()\n",
    "                    response_text = f\"Query Generated: {response_text}\\nOutput: {str(results)}\\n\"\n",
    "                    success = True\n",
    "                except mysql.connector.Error as err:\n",
    "                    print(f\"Query execution failed with entropy with error: {err}, {response_text}\")\n",
    "                except Exception as entropy_error:\n",
    "                    print(f\"Entropy-based retry failed with error: {entropy_error}\")\n",
    "    # If no success after max attempts, notify the user\n",
    "    if not success:\n",
    "        responseText = f\"Failed to execute query after {maxAttempts} attempts.\"\n",
    "\n",
    "    return responseText\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt 1: Considering top 2 tables\n",
      "Top tables after re-ranking: ['club_list', 'club_head_details']\n",
      "Response: Query Generated:  SELECT c.club_name, h.club_head FROM club_list AS c JOIN club_head_details AS h ON c.club_id = h.head_id;\n",
      "Output: [('AeroModeling Club', 'John Doe'), ('Animal Welfare Club', 'Jane Smith'), ('Anti Drug Club', 'Sam Johnson'), ('Artificial Intelligence & Robotics', 'Emily Davis'), ('Association of Serious Quizzers', 'Chris Brown'), ('Astronomy Club', 'Anna Lee'), ('Book Readers Club', 'Michael White'), ('CAP Nature Club', 'Emma Wilson'), ('Cyber Security Club', 'David Harris'), ('Dramatix Club', 'Sophia Thompson'), ('English Literary Society', 'James Martin'), ('Entrepreneurs Club', 'Olivia Taylor'), ('Fine Arts Club', 'Benjamin Walker'), ('Finverse Club', 'Ava Scott'), ('Global Leaders Forum', 'Liam Adams'), ('Higher Education Forum', 'Isabella Nelson'), ('Industry Interaction Forum', 'Noah Young'), ('Martial Arts Club', 'Mia Allen'), ('PSG Tech Chronicle Club', 'Lucas Hall'), ('Paathshala Club', 'Charlotte King'), ('Radio Hub', 'Mason Wright'), ('Rotaract Club', 'Amelia Moore'), ('SPIC-MACAY Heritage Club', 'Ethan Green'), ('Student Research Council', 'Harper Lewis'), ('Tech Music', 'Alexander Clark'), ('Women Development Cell', 'Abigail Turner'), ('Youth Outreach Club', 'Elijah Baker'), ('Youth Red Cross Society', 'Evelyn Phillips'), ('Yuva Tourism Club', 'Logan Mitchell')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "inputText = \"List the names of the club heads and the clubs they belong to.\"\n",
    "reply = processUserQuery(inputText)\n",
    "print(f\"Response: {reply}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
